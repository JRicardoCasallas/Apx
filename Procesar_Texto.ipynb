{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JRicardoCasallas/Apx/blob/main/Procesar_Texto.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "v1WE418laX3X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LqlOmjIGuO5",
        "outputId": "8770b10a-cca6-41c7-f8dc-7884674a6eef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk # Instala la biblioteca NLTK para procesamiento de lenguaje natural"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zocc8VvbHAuI",
        "outputId": "dea1739b-4d0b-4bae-dcb7-da145fae576e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "import nltk#Importa la librería nltk para procesamiento de lenguaje natural.\n",
        "import re#Importa la librería re para expresiones regulares.\n",
        "nltk.download('punkt')# Descarga el recurso 'punkt' para tokenización (dividir texto en oraciones y palabras).\n",
        "nltk.download('stopwords')# Descarga la lista de 'stopwords' (palabras comunes que se suelen eliminar en el análisis).\n",
        "nltk.download('wordnet')# Descarga 'wordnet', una base de datos léxica para análisis semántico.\n",
        "nltk.download('punkt_tab')# Descarga 'punkt_tab', un recurso adicional para tokenización usando tabulaciones.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "umSxT4SuIEiP"
      },
      "outputs": [],
      "source": [
        "def calcular_vocabulario(documentos): # Define una función llamada calcular_vocabulario que toma una lista de documentos como entrada.\n",
        "    vocabulario = []  # Inicializa una lista vacía llamada 'vocabulario' para almacenar todas las palabras.\n",
        "    for element in documentos:  # Itera a través de cada documento en la lista 'documentos'.\n",
        "        vocabulario.extend(element)  # Agrega todas las palabras del documento actual a la lista 'vocabulario'.\n",
        "    vocabilario_dic = set(vocabulario)  # Convierte la lista 'vocabulario' en un conjunto para eliminar palabras duplicadas.\n",
        "    return len(vocabilario_dic)  # Devuelve la longitud del conjunto, que representa el tamaño del vocabulario."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "documentos = [...]:Esta línea declara una variable nombrada documentos y la asigna a una lista.\n",
        "[...]:Los corchetes definen una lista en Python.\n",
        "Cada cadena dentro de los corchetes : Cada cadena dentro de las comillas (\"...\") representa un documento o fragmento de texto independiente. La lista documentoscontiene 8 de estas cadenas de texto, todas relacionadas con Colombia.\n",
        "En esencia, este código almacena una colección de textos sobre Colombia en una lista de Python para su posterior procesamiento o análisis por parte de las otras partes del código."
      ],
      "metadata": {
        "id": "KL8l2XAHesWy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "fUehzrlNGzIV"
      },
      "outputs": [],
      "source": [
        "documentos =  [ # Esta línea crea una lista llamada 'documentos' para almacenar textos sobre Colombia.\n",
        "    \"Colombia, oficialmente República de Colombia, es un país soberano situado en la región noroccidental de América del Sur\",\n",
        "    \"Es la única nación de América del Sur que tiene costas en el océano Pacífico y acceso al Atlántico a través del mar Caribe\",\n",
        "    \"Es el vigesimoctavo país más poblado del mundo, con una población de 51 millones de habitantes\",\n",
        "    \"La presencia humana en Colombia se remonta a más de 10.000 años\",\n",
        "    \"Colombia tiene una economía diversificada y posee un importante componente de servicios\",\n",
        "    \"Es la segunda nación más biodiversa del mundo, contando con 54871 especies registradas\",\n",
        "    \"La denominación de Colombia proviene del apellido del explorador genovés del siglo XV Cristóbal Colón (en italiano: Cristoforo Colombo, en latín Christophorus Columbus)\",\n",
        "    \"El estudio de los primeros pobladores del territorio que hoy comprende la Nación se ha dividido en tres etapas de la época precolombina\",\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3Hs8moxcemy2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokeniza cada documento en 'documentos':Esto explica que la línea de código está tokenizando (dividiendo en palabras y puntuación) cada documento almacenado dentro de la variable documentos.\n",
        "y almacena los tokens en 'tokenize_documents':Esto aclara que los tokens resultantes se almacenan en una nueva variable denominada tokenize_documents."
      ],
      "metadata": {
        "id": "7pEQ9FSkgroF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "QPOgGoh0G-uA"
      },
      "outputs": [],
      "source": [
        "tokenize_documents = [nltk.word_tokenize(d) for d in documentos]\n",
        "# Tokeniza cada documento en 'documentos' y almacena los tokens en 'tokenize_documents'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gogv3nTMIKSr",
        "outputId": "5b192576-7597-4543-eb34-5441cac0726d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "96"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "calcular_vocabulario(tokenize_documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En esencia, la línea calcular_vocabulario(tokenize_documents)activa el cálculo del tamaño del vocabulario utilizando los documentos procesados ​​y devuelve el número total de palabras únicas encontradas."
      ],
      "metadata": {
        "id": "Ed0c9OISiCwM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "EAg3nCkvIPYe"
      },
      "outputs": [],
      "source": [
        "#Eliminar caracteres espciales\n",
        "\n",
        "def preprocesor(documento): # Define una función llamada 'preprocesor' que toma un 'documento' como entrada.\n",
        "    patron = \"[(),.:!]\"  # Define un patrón de caracteres especiales a eliminar usando una expresión regular.\n",
        "    text_pro = re.sub(patron, \"\", documento)# Reemplaza los caracteres especiales encontrados con una cadena vacía.\n",
        "    return text_pro.strip()# Devuelve el texto procesado sin espacios en blanco al inicio o al final.\n",
        "\n",
        "documentos_procesados = []#Crea una lista vacía para almacenar los documentos procesados.\n",
        "\n",
        "for doc in documentos: #Itera a través de cada documento en la lista 'documentos'.\n",
        "    doc_preprocesado = preprocesor(doc)#Llama a la función 'preprocesor' para limpiar el documento actual.\n",
        "    documentos_procesados.append(doc_preprocesado)#Agrega el documento limpio a la lista 'documentos_procesados'.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "p1ELCGwcI0xc"
      },
      "outputs": [],
      "source": [
        "tokenize_documents_sin_catacteres = [nltk.word_tokenize(d) for d in documentos_procesados]\n",
        "#Tokeniza cada documento en 'documentos_procesados' y almacena los tokens en 'tokenize_documents_sin_catacteres'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Esta línea utiliza una técnica denominada \"comprensión de listas\" para procesar de manera eficiente una lista de documentos. Esto es lo que sucede paso a paso:\n",
        "\n",
        "documentos_procesados: Esta es una lista, que probablemente contiene documentos de texto que ya han pasado por algún preprocesamiento (como la eliminación de caracteres especiales como se ve en el código anterior).\n",
        "\n",
        "for d in documentos_procesados: Esta parte recorre en iteración cada documento individual dentro de la documentos_procesadoslista. Cada documento se asigna temporalmente a la variable dpara su procesamiento dentro del bucle.\n",
        "\n",
        "nltk.word_tokenize(d): Aquí es donde ocurre la tokenización.\n",
        "\n",
        "nltkEs la biblioteca del kit de herramientas de lenguaje natural.\n",
        "word_tokenizees una función nltkque divide una cadena de texto ( den este caso) en palabras individuales y signos de puntuación, llamados \"tokens\".\n",
        "[...]: Los corchetes crean una nueva lista. Los resultados de nltk.word_tokenize(d)cada documento se recopilan y almacenan como elementos en esta nueva lista.\n",
        "\n",
        "tokenize_documents_sin_catacteres = ...: Finalmente, esta nueva lista (que contiene las versiones tokenizadas de todos los documentos) se asigna a la variable tokenize_documents_sin_catacteres.\n",
        "\n",
        "En términos más simples:\n",
        "\n",
        "Esta línea de código toma una lista de documentos procesados, divide cada documento en palabras y signos de puntuación individuales y almacena estos documentos tokenizados en una nueva lista llamada tokenize_documents_sin_catacteres. Los documentos originales permanecen sin cambios."
      ],
      "metadata": {
        "id": "-7MsNQ7AkIRl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wi1Y3XXnJBr-",
        "outputId": "fd582f91-b1ab-4c9d-96c4-c1fa8e9d0edf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "74"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "#Eliminar palabras de parada #Comentario que indica el objetivo de este bloque de código: eliminar palabras de parada\n",
        "\n",
        "diccionario_stop_words = (nltk.corpus.stopwords.words('spanish')) # Obtiene la lista de palabras de parada en español de NLTK y la guarda en 'diccionario_stop_words'\n",
        "\n",
        "documentos_sin_palabras_pabrada = [] # Crea una lista vacía para almacenar los documentos sin palabras de parada\n",
        "\n",
        "for document in tokenize_documents_sin_catacteres: # Itera sobre cada documento en la lista 'tokenize_documents_sin_catacteres'\n",
        "  document = [word for word in document if word not in diccionario_stop_words] # Usa una comprensión de lista para filtrar las palabras de parada de cada documento\n",
        "  documentos_sin_palabras_pabrada.append(document) # Agrega el documento filtrado a la lista 'documentos_sin_palabras_pabrada'\n",
        "\n",
        "calcular_vocabulario(documentos_sin_palabras_pabrada) # Llama a la función 'calcular_vocabulario' para calcular el vocabulario de los documentos sin palabras de parada\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ykXX59WwJLDU",
        "outputId": "e997b1ab-b141-4c75-f895-3a13fde36bf3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['Colombia', 'oficialmente', 'República', 'Colombia', 'país', 'soberano', 'situado', 'región', 'noroccidental', 'América', 'Sur'], ['Es', 'única', 'nación', 'América', 'Sur', 'costas', 'océano', 'Pacífico', 'acceso', 'Atlántico', 'través', 'mar', 'Caribe'], ['Es', 'vigesimoctavo', 'país', 'poblado', 'mundo', 'población', '51', 'millones', 'habitantes'], ['La', 'presencia', 'humana', 'Colombia', 'remonta', '10000', 'años'], ['Colombia', 'economía', 'diversificada', 'posee', 'importante', 'componente', 'servicios'], ['Es', 'segunda', 'nación', 'biodiversa', 'mundo', 'contando', '54871', 'especies', 'registradas'], ['La', 'denominación', 'Colombia', 'proviene', 'apellido', 'explorador', 'genovés', 'siglo', 'XV', 'Cristóbal', 'Colón', 'italiano', 'Cristoforo', 'Colombo', 'latín', 'Christophorus', 'Columbus'], ['El', 'estudio', 'primeros', 'pobladores', 'territorio', 'hoy', 'comprende', 'Nación', 'dividido', 'tres', 'etapas', 'época', 'precolombina']]\n"
          ]
        }
      ],
      "source": [
        "docuemt = []  # Crea una lista vacía llamada 'docuemt' para almacenar los documentos sin palabras de parada.\n",
        "for document in tokenize_documents_sin_catacteres:  # Itera a través de cada documento en la lista 'tokenize_documents_sin_catacteres'.\n",
        "  list_words = []  # Crea una lista vacía llamada 'list_words' para almacenar las palabras que no son palabras de parada del documento actual.\n",
        "  for word in document:  # Itera a través de cada palabra en el documento actual.\n",
        "    if word not in diccionario_stop_words:  # Verifica si la palabra actual no está en la lista de palabras de parada.\n",
        "      list_words.append(word)  # Si la palabra no es una palabra de parada, se agrega a la lista 'list_words'.\n",
        "  docuemt.append(list_words)  # Agrega la lista 'list_words' (que contiene las palabras que no son palabras de parada) a la lista 'docuemt'.\n",
        "print(docuemt)  # Imprime la lista 'docuemt', que contiene los documentos sin palabras de parada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgjW0AF1KbN7",
        "outputId": "cc4b59b4-7398-4218-f4cd-5bb2913e47d4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "23"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "# Calcula el vocabulario del último documento procesado en el bucle anterior.\n",
        "# 'document' contiene la última lista de palabras del bucle anterior\n",
        "# y la función 'calcular_vocabulario' cuenta las palabras únicas en esa lista.\n",
        "calcular_vocabulario(document)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "u3VTob8bKnC6"
      },
      "outputs": [],
      "source": [
        "#Stemming - Esta sección del código realiza el stemming en los documentos.\n",
        "\n",
        "p_stemmer = nltk.stem.porter.PorterStemmer() # Crea un objeto PorterStemmer para realizar stemming en las palabras.\n",
        "                                               # PorterStemmer es un algoritmo basado en reglas para reducir las palabras a su raíz.\n",
        "\n",
        "documentos_stemmig = [] # Inicializa una lista vacía para almacenar los documentos después del stemming.\n",
        "\n",
        "for document in documentos_sin_palabras_pabrada: # Itera a través de cada documento en la lista 'documentos_sin_palabras_pabrada',\n",
        "                                                 # que presumiblemente contiene documentos sin palabras de parada.\n",
        "  document = [p_stemmer.stem(word) for word in document] # Aplica el stemmer a cada palabra en el documento actual utilizando una comprensión de lista.\n",
        "                                                       # Esto crea una nueva lista con las palabras derivadas y la asigna de nuevo a 'document'.\n",
        "  documentos_stemmig.append(document) # Agrega el documento con stemming a la lista 'documentos_stemmig'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bE_3FZMHM3Jz",
        "outputId": "5758ecec-7715-4265-ad09-0cadb3f181a1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "73"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "calcular_vocabulario(documentos_stemmig) # Calcula el tamaño del vocabulario de los documentos después del stemming. Utilice el código con precaución"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "xEk3nOUmMHOH"
      },
      "outputs": [],
      "source": [
        "# Lematización  # Esta sección del código realiza la lematización\n",
        "word_lemmatizer = nltk.stem.WordNetLemmatizer() # Crea un objeto WordNetLemmatizer para la lematización\n",
        "docuementos_lematizador = [] # Inicializa una lista vacía para almacenar los documentos lematizados\n",
        "for document in documentos_sin_palabras_pabrada: # Itera a través de cada documento en la lista preprocesada\n",
        "  document = [word_lemmatizer.lemmatize(word) for word in document] # Lematiza cada palabra en el documento usando una comprensión de lista\n",
        "  docuementos_lematizador.append(document) # Agrega el documento lematizado a la lista"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sm7F4oLNNpQz",
        "outputId": "0ce234a6-58c7-4c46-f006-af08fe4ed12b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "74"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "calcular_vocabulario(docuementos_lematizador) # Calcula el tamaño del vocabulario de los documentos después de la lematización."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}